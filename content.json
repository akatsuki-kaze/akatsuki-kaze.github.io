{"meta":{"title":"akatsuki kaze's blogs","subtitle":"","description":"a personal blog","author":"akatsuki kaze","url":"http://akatsuki-kaze.com.cn","root":"/"},"pages":[{"title":"categories","text":"RC往事这是robocon历史上（可能）第一个记录了很多robocon历史往事的仓库。所有人均可上传和访问。 MTI战队2025届ROBOCON排球机械以及部分上位机算法开源关于2025届排球的部分方案开源，包括迭代前的上位机算法和R1机械开源 一个图像二值化小玩具字面意思，可以快速选取二值化范围的小工具，也许某些时候会有点用处？","path":"categories/index.html","date":"11-19","excerpt":""},{"title":"tags","text":"","path":"tags/index.html","date":"11-20","excerpt":""},{"title":"about","text":"我是谁我是来自福建理工大学MTI战队的晓风，在战队中担任算法组组长，主要负责上位机算法的开发。 如果您想要与我联系QQ：1652107474E-mail:1652107474@qq.comgithub:akatsuki kazebilibili: 晓风是男孩子","path":"about/index.html","date":"11-19","excerpt":""}],"posts":[{"title":"the_past_of_curc_robocon","text":"RC往事rc历史上可能是第一个记录了“技术大佬”们鲜活日常的仓库","path":"2025/11/20/the-past-of-robocon/","date":"11-20","excerpt":"","tags":[{"name":"robocon","slug":"robocon","permalink":"http://akatsuki-kaze.com.cn/tags/robocon/"}]},{"title":"coworkers_for_KFS","text":"KFS数据集共创计划[TOC] 前言PS. 本文项目现已刊登至比赛组委会官方公众号 原文链接 在2026赛季的robocon比赛中，我们可以发现机器人在功夫卷轴上完成识别几乎是硬性需求，但是功夫卷轴种类繁多，光凭借一个队伍的视觉ldx和xdx，每种50张工作量也是巨大的。因此，本人决定发起一个数据集标注活动，将每种图片规定好标签等，然后汇总到github仓库，并且通过一些开源中附带的小工具针对不同队伍需求进行数据格式转换。同时保留了原始数据文件夹供各个队伍自行决定。由于采用yolo模型的人数较多，因此本人倡议使用yolo数据集格式（即txt坐标文件+原始图像+一个标签文件）。本项目不要求区分train和val，只需要标注完成后上传即可。希望Rc比赛可以有更好的开源氛围以及共创氛围。20251110更新：增加了马术分支，但是由于本人精力有限，目前需要助手帮助维护更新马术分支 一、数据采集注意事项由于比赛过程中，我们可能需要面对复杂的场景以及灯光干扰，因此本人建议在数据集中尽可能涉及大块纯色色块（绿色、红色或者蓝色）。可以考虑加入一些天花板为背景的图片作为应对灯光的抗干扰。若觉得模型数据仍然不理想，可以参考我校排球开源中的灯光抗干扰方案，但是未经验证不知可行性。数据集采集时，可以通过录制1080p30帧的视频，然后导入剪辑软件中。剪辑软件设置为导出640*640（yolo默认格式）视频，随后在格式工厂（或者其他工具中）导出为帧，然后摇两个小登开始标数据。 注：关于剪辑软件说明个人推荐：达芬奇(专业，免费的剪辑软件，适合让运营兼职的同时学习，也适合想要学习新技能的同学。同时本身也有强大的各种工具。如果有需要本人后续会录制简单说明)剪映(相对达芬奇，简单易上手，专业性并没有差很多，十分适合所有视觉组成员学习一下)finalcut(macOS专用，但是本人用的不多)PR为付费软件，处于支持正版角度考虑不建议购买使用，并且对于新手没有达芬奇好上手。必剪，本人不是很推荐的一款软件，但是如果有会使用的同学可以考虑使用。其余软件本人使用不多，是否采用还是以个人习惯为主看，本人仅以高中以及大一时期融媒体工作流相关经验提出观点。 二、数据集标注要求（一）标签要求&ensp;标签从零到一到32开始，并且每张图最好只出现一类标签！标注时，请框选整个KFS（若条件有限则只框选有颜色的部分）。标签顺序如图标注：上为甲骨、下为小篆，内容一致：武林至尊戈刀弓矢力德王帝天日月左上角的数字代表标签顺序（本来应从零开始，但是由于外包疏忽以及理解方便还是标注为1开始计数），标签建议在制作数据集之前就提前创建好或者使用本项目下的标签文件（targets文件夹内）。接下来会规范每一个标签的命名，从第一个开始： R_R1 B_R1 T_03 T_04 T_05 T_06 T_07 T_08 T_09 T_10 T_11 T_12 T_13 T_14 T_15 T_16 T_17 F_18 F_19 F_20 F_21 F_22 F_23 F_24 F_25 F_26 F_27 F_28 F_29 F_30 F_31 F_32 本意是可以让网络识别到单个字体，后续通过标签来判断真假。标注时请注意类别一定要全部添加，后续整理合并数据集时才不会混乱。 （二）关于数据集&ensp;如前文所说，本人倡议使用yolo数据集格式，因为本人接触到的队伍大多都使用yolo作为主要识别模型，因此本项目推荐使用yolo格式。若有其他格式需求可以使用转换工具。如果有需求，可以单独开设数据集格式文件夹以供使用。数据集最小数量要求为50张。 三、标注完成（一）整理数据集&ensp;将您的数据集标签与图片放置于同一文件夹下，命名为贵校缩写或者战队英文名（例如FJUT，MTI等），然后上传至对应的文件夹下。 （二）上传文件&ensp;提交PR至对应文件夹，文件夹结构如下 --+--labeled | +--orignal_datas | +--tools labeled为数据集对应存放处，打开即可看见多种数据集名字。请按照上述要求创建好学校名字文件夹后提交您的pr。如果发现pr没有及时采用请私聊骚扰本人。如果您有需求也可以通过悬赏提出issue，也请各位创作时注意数据集的均衡性，尽量保证r2的每一种方块数据数量是相等的。 四、下载数据集（一）下载&ensp;您可以通过各种方式下载，这里不再赘述 （二）使用&ensp;在本项目的tools文件夹下会有各种文件处理工具，具体使用说明请参阅tools下的readme文档。由于部分工具可能非本人上传，因此请谨慎使用或检查源码自行评估使用。上传的工具必须为c源码或python的py程序，上传工具源码必须可见！这是根本原则！在进行完数据集合并后，我建议您自行半随机抽取图像进行数据集的划分，因为不同学校拍摄环境不同，训练效果也不同，若均匀分配则可以使模型泛化能力更强。具体如何操作看您设计。 五、如何参与获取联系方式或参与讨论：&ensp;添加个人联系方式：1652107474（QQ）或者进入QQ群1065597020。 参与协作：&ensp;将您标注的数据集的图片（总览）以及数据集通过邮箱发给本人（1652107474@qq.com）,若您介意，只需要发送标注好的部分图像也可，本人将会在三个工作日内完成审核并且邀请您加入私有仓库协作，请在发送邮件时注明学校以及参加KFS标注协作，若审核不及时请在群内艾特一下本人。 六、碎碎念&ensp;由于这是多人共创项目，并且由于加入时间先后，队员数量等原因必然会产生各个学校上传数据集不同等的现象。例如本人队伍目前只有本人一位正式的视觉组成员，因此产出数据集必然是不同等的，如果您需要根据已有总量评估，请联系本人，我也会尽量配合您进行数据集数量评估。我也并未强制要求您将所有数据集上传，仅需要上传您认为值得上传的部分，希望参与本项目的学校可以对此做好心理准备&ensp;我仍然衷心希望RC可以是一个开放包容的比赛氛围。对于贡献较高的队伍在比赛时我们将会为其准备一些神秘小礼品以示心意。本项目仍然在完善阶段，欢迎踊跃参与。","path":"2025/11/20/coworkers-for-KFS/","date":"11-20","excerpt":"","tags":[{"name":"robocon","slug":"robocon","permalink":"http://akatsuki-kaze.com.cn/tags/robocon/"}]},{"title":"MTI_volleyball_opensource","text":"MTI战队视觉方案开源前言 在初步研究了排球比赛的规则之后，我们组内成员偏向于认为视觉方案在这次比赛中可能会有一个比较重要的作用。但是在实际比赛中，我们发现部分队伍因为灯光影响而放弃视觉方案。部分学校则是采取了风格特异的抗干扰方案排除灯光影响。例如某大学是采取了现场标记数据集，而另一个大学则是数据集一开始就使用蓝色为主的排球，由此达成抗干扰效果。尽管由于其他问题我们的视觉识别并未起到作用，但是鉴于实地测试表现我决定仍然将其开源。电控部分代码则开源在这个仓库。本代码在这里开源,在闲鱼购买请联系原作者1652107474@qq.com 一、代码功能综述 本代码结构为d415_ubuntu.py作为主程序，anti_light.py作为抗灯光部分，Serial_akatsuki_simple.py作为串口通讯相关的函数处理（包括数据内容以及编码发送），pid_akatsuki.py是用于跟踪的pid控制器。run.bat是为win环境准备的启动程序（win版本主程序为d415）。本代码不依赖ros，需求python版本3.9及以上，3.10经过测试稳定运行。要求库已经内置在requirementa.txt中。ubuntu需要使用python命令启动。程序启动时需要接入xinput支持的手柄，realsense d4系列带rgb摄像头（d410可以使用深度摄像头传输rgb流，d430不行），以及ch340（我们的方案是自带ch340的nrf24射频） 二、主函数代码解析 12行开始代码部分 pipeline = rs.pipeline() #定义流程pipeline config = rs.config() #定义配置config config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 60) #配置depth流 config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 60) #配置color流 profile = pipeline.start(config) #流程开始 align_to = rs.stream.color #与color流对齐 align = rs.align(align_to) 以上是realsense相关的初始化 # 全局变量,用于存储模型和摄像头对象 model = None cap = None track = 0 act_model = 0 # 用于控制动作的变量 X,Y,RIGHT_X,RIGHT_Y= 0,0,0,0 # 控制移动的变量 # 设置模型配置 script_dir = Path(__file__).parent file_path = script_dir / &#39;model&#39; / &#39;yolo11s.pt&#39; model_config = { &#39;model_path&#39;: str(file_path), &#39;download_url&#39;: &#39;https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt&#39; } # 推理参数 predict_config = { &#39;conf_thres&#39;: 0.6, &#39;iou_thres&#39;: 0.2, &#39;imgsz&#39;: 640, &#39;line_width&#39;: 2, &#39;device&#39;: &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39; } def init_inference(confidence, iou): global model # 加载 YOLO 模型 model = YOLO(model_config[&#39;model_path&#39;]).to(predict_config[&#39;device&#39;]) return True 这里是关于yolo推理部分的初始化，其中X,Y,RIGHT_X,RIGHT_Y= 0,0,0,0会在稍后作为传入信息数组的变量，提前进行全局声明和初始化。model_config是模型的路径，仍然保留了网络下载路径，但是实际代码中没有相关功能。init_inference函数用于初始化模型，若初始化成功会返回true。接下来直接转到识别部分 global frame1,rgb,center1 # 读取摄像头的帧 frame1 = rgb.copy() # 使用 YOLO 模型进行推理,并传入置信度和 IoU 阈值 results1 = model(frame1, conf=0.5, iou=0.7) detect_len = len(results1[0].boxes) #获取检测结果的长度 # 获取检测结果的坐标并绘制检测框 boxes1 = [] if detect_len != 0: for r in results1: for box in r.boxes: xyxy = box.xyxy[0].cpu().numpy().astype(int) boxes1.append(xyxy) cv2.rectangle(frame1, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2) center1 = np.array([x_center1, y_center1]) else: print(&quot;未检测到目标&quot;) center1 = np.array([320, 5]) 这段代码对应yolo的识别部分。由于anti_light代码在nuc上，后续会惊醒补充。接下来来到代码的主函数部分，108行开始： global actX,act_dis x_center1 = 320 y_center1 = 240 center1 = np.array([x_center1, y_center1]) actX = pid.ProportionalPID( kd= 260, ki= 0, kp= 180, deadband= 0.01 ) act_dis = pid.ProportionalPID( kd= 220, ki= 0, kp= 160, deadband= 0.001 ) 提前声明部分变量，实例化需要用到的两个pid控制器（对应水平和竖直方向）接下来是串口选择部分，这个部分目前只有win版本有效，ubuntu版本已经删除。选择串口时，用手柄的hat（十字键）进行选择，上下为加减5，左右加减1（ubuntu版本功能为上下加减1）。选择完成后，按下手柄A按键确定串口。170行的asyncio.run(detect()) #协程推理使用了asyncio库进行协程，达到优化推理产生的控制延迟的效果。实际上发现在一定程度上也优化了推理性能。173行开始的这段代码中 x = int(center1[0]) y = int(center1[1]) #(x, y)点的真实深度值 dis = aligned_depth_frame.get_distance(x, y) #(x, y)点在相机坐标系下的真实值,为一个三维向量。其中camera_coordinate[2]仍为dis,camera_coordinate[0]和camera_coordinate[1]为相机坐标系下的xy真实距离。 camera_coordinate = rs.rs2_deproject_pixel_to_point(depth_intrin, [x, y], dis) if camera_coordinate != (0.0,0.0,0.0): last_coordinate = camera_coordinate print(camera_coordinate) else: print(last_coordinate) 这段代码会对像素坐标进行获取。由于深度摄像头在距离过近的目标识别会输出0，0，0的坐标值，因此会对目标最后的位置进行检测，决定是否写入坐标值 接下来是198行，对应动作组 # 执行动作组 if sc.joystick.get_button(13) == 1: if camera_coordinate[2] !=0.0: act_group = 1 else: act_group = 2 else: act_group = 0 action_group(act_group) act_group = 0 这段代码在手柄左摇杆按下时会进入自动跟踪状态，同时通过目标不同状态选择动作组。在执行程序action_group结束后act_group会归为0，防止动作重复执行。 sc.time.sleep(0.01) robot.send() 这段为发送所有的数据组，sleep为阻塞线程，防止串口发送频率太快导致报错","path":"2025/11/19/posts/","date":"11-19","excerpt":"","tags":[{"name":"robocon","slug":"robocon","permalink":"http://akatsuki-kaze.com.cn/tags/robocon/"}]}],"categories":[],"tags":[{"name":"robocon","slug":"robocon","permalink":"http://akatsuki-kaze.com.cn/tags/robocon/"}]}